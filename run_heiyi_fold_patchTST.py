import warnings
warnings.filterwarnings(
    "ignore",
    "The pynvml package is deprecated. Please install nvidia-ml-py instead.",
    FutureWarning
)
import argparse
import os
import torch
from exp.exp_multiple_regression_fold_time import Exp_Multiple_Regression_Fold
import random
import numpy as np
# from src.model_phi_heiyi import phi  # åŠ è½½phi
import joblib
import os
# ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œè®­ç»ƒ5æŠ˜
from multiprocessing import Process, set_start_method
import torch.multiprocessing
import time
import subprocess
import sys



os.environ["KMP_AFFINITY"] = "noverbose"

parser = argparse.ArgumentParser(description='phi2')

# basic config
parser.add_argument('--task_name', type=str, default='multiple_regression',
                    help='task name, options:[Long_term_forecasting, anomaly_detection, predict_feature,multiple_regression, LGB]')
parser.add_argument('--is_training', type=int, default=1, help='status')
parser.add_argument('--model_id', type=str, default='test', help='model id')
parser.add_argument('--model', type=str, default='FC_MLP',
                    help='model name, options: [GPT2TS, ]') # PatchTST_multi_scale

# data loader
parser.add_argument('--dataset', type=str, default='ETTh1',
                    help='[ETTh1, ETTh2, ETTm1, ETTm2, weather, psm, smap]')
parser.add_argument('--prompt',type=str, default='Etth1')
parser.add_argument('--root_path', type=str, default='/home/liangxijie1/phi-2/dataset/',
                    help='root path of the data file:feature_1419_5, d1')
parser.add_argument('--data_path', type=str, default='LongtermForecast/ETT-small/',
                    help='data file, options: [ETT-small, electricity, exchange_rate, illness, traffic, weather]')
parser.add_argument('--freq', type=str, default='t',
                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')
parser.add_argument('--features', type=str, default='S',
                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')
parser.add_argument('--checkpoints', type=str, default='./checkpoints_heiyi/', help='location of model checkpoints')

parser.add_argument('--drop_ratio', type=float, default=0.2, help='Set a dropping ratio for feature_selection')
parser.add_argument('--train_data_start_year', type=int, default=2010)
parser.add_argument('--test_data_start_year', type=int, default=2021)
parser.add_argument('--feature_selection',type = bool, default=False, help='whether to use feature selection')
parser.add_argument('--extra_input',type = bool, default=False, help='whether to add tikcter')

# Forecast task
parser.add_argument('--seq_len', type=int, default=64, help='input sequence length')
parser.add_argument('--pred_len', type=int, default=1, help='prediction sequence length')

# phi-2
parser.add_argument('--block_size', type=int, default=1024)
parser.add_argument('--n_layer', type=int, default=6)
parser.add_argument('--n_head', type=int, default=12)
parser.add_argument('--n_embd', type=int, default=768)
parser.add_argument('--embd_pdrop', type=float, default=0.1)
parser.add_argument('--resid_pdrop', type=float, default=0.1)
parser.add_argument('--attn_pdrop', type=float, default=0.1)
parser.add_argument('--patch_len', type=int, default=32)
parser.add_argument('--stride', type=int, default=4)
parser.add_argument('--individual', action='store_true', help='use automatic mixed precision training', default=False)
parser.add_argument('--r', type=int, default=8)

# model define
parser.add_argument('--expand', type=int, default=2, help='expansion factor for Mamba')
parser.add_argument('--d_conv', type=int, default=4, help='conv kernel size for Mamba')
parser.add_argument('--top_k', type=int, default=5, help='for TimesBlock')
parser.add_argument('--num_kernels', type=int, default=6, help='for Inception')
parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')
parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')
parser.add_argument('--c_out', type=int, default=1, help='output size')
parser.add_argument('--d_model', type=int, default=64, help='dimension of model')
parser.add_argument('--n_heads', type=int, default=8, help='num of heads')
parser.add_argument('--e_layers', type=int, default=3, help='num of encoder layers')
parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')
parser.add_argument('--d_ff', type=int, default=32, help='dimension of fcn')
parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')
parser.add_argument('--factor', type=int, default=1, help='attn factor')
parser.add_argument('--distil', action='store_false',
                    help='whether to use distilling in encoder, using this argument means not using distilling',
                    default=True)
parser.add_argument('--dropout', type=float, default=0.1, help='dropout')
parser.add_argument('--embed', type=str, default='timeF',
                    help='time features encoding, options:[timeF, fixed, learned]')
parser.add_argument('--activation', type=str, default='gelu', help='activation')
parser.add_argument('--channel_independence', type=int, default=0,
                    help='0: channel dependence 1: channel independence for FreTS model')
parser.add_argument('--decomp_method', type=str, default='moving_avg',
                    help='method of series decompsition, only support moving_avg or dft_decomp')
parser.add_argument('--use_norm', type=int, default=1, help='whether to use normalize; True 1 False 0')
# parser.add_argument('--down_sampling_layers', type=int, default=0, help='num of down sampling layers')
# parser.add_argument('--down_sampling_window', type=int, default=1, help='down sampling window size')
# parser.add_argument('--down_sampling_method', type=str, default=None,
#                     help='down sampling method, only support avg, max, conv')
parser.add_argument('--seg_len', type=int, default=48,
                    help='the length of segmen-wise iteration of SegRNN')
# LGB
parser.add_argument('--feature_path', type=str, default='/home/dmz-ai/liruoling/heiy/results/fea/PatchTST', help='npy')

# MLP
parser.add_argument('--MLP_hidden', type=int, default=32,
                    help='The middle tier scale of fc MLPn in ecoder')
parser.add_argument('--MLP_layers', type=int, default=2, help='layers of MLP')
parser.add_argument('--kernel_size', type=int, default=7, help='kernel size of fc conv')
parser.add_argument('--max_depth', type=int, default=2, help='kernel size of fc conv')
parser.add_argument('--weight_std', type=float, default=0.01, help='weight initializes standard deviation')

# timeMixer
parser.add_argument('--down_sampling_layers', type=int, default=3, help='num of down sampling layers')
parser.add_argument('--down_sampling_window', type=int, default=2, help='down sampling window size')
parser.add_argument('--down_sampling_method', type=str, default='avg',
                    help='down sampling method, only support avg, max, conv')
# Client
parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')
parser.add_argument('--w_lin', type=float, default=1.0, help='initial weight of the linear model')
# Fredformer
parser.add_argument('--cf_dim',         type=int, default=640)   #feature dimension
parser.add_argument('--cf_drop',        type=float, default=0.2)#dropout
parser.add_argument('--cf_depth',       type=int, default=3)    #Transformer layer
parser.add_argument('--cf_heads',       type=int, default=8)    #number of multi-heads
#parser.add_argument('--cf_patch_len',  type=int, default=16)   #patch length
parser.add_argument('--cf_mlp',         type=int, default=640)  #ff dimension
parser.add_argument('--cf_head_dim',    type=int, default=32)   #dimension for single head
parser.add_argument('--cf_weight_decay',type=float, default=0)  #weight_decay
parser.add_argument('--cf_p',           type=int, default=1)    #patch_type
parser.add_argument('--use_nys',           type=int, default=1)    #use nystrom
parser.add_argument('--mlp_drop',           type=float, default=0.3)    #output type
parser.add_argument('--ablation',       type=int, default=0)    #ablation study 012.
parser.add_argument('--fc_dropout', type=float, default=0.05, help='fully connected dropout')
parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')
parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')
parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')
parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')
parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')
# parser.add_argument('--mlp_hidden', type=int, default=64, help='hidden layer dimension of model')
# CycleNet.
parser.add_argument('--cycle', type=int, default=24, help='cycle length')
parser.add_argument('--model_type', type=str, default='mlp', help='model type, options: [linear, mlp]')
# optimization
parser.add_argument('--num_workers', type=int, default=8, help='data loader num workers')
parser.add_argument('--train_epochs', type=int, default=20, help='train epochs')
parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')
parser.add_argument('--early_open', type=bool, default=True)
parser.add_argument('--patience', type=int, default=5, help='early stopping patience')
parser.add_argument('--learning_rate', type=float, default=0.005, help='optimizer learning rate')
parser.add_argument('--optim_type', type=str, default='Adam', help='select optimizer type, optional[SGD, Adam]')
parser.add_argument('--weight_decay', type=float, default=0.0001, help='weight decay value')
parser.add_argument('--loss', type=str, default='MSE_with_weak', help='loss function, optional[ MSE, MAE, CCC]')
parser.add_argument('--lradj', type=str, default='type1',
                    help='adjust learning rate, optional:[type1, type2, not, cos, steplr]')
parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)
parser.add_argument('--clip_value', type=float, default=0.5, help='clip grad')
parser.add_argument('--pct_start', type=int, default=0.6)
# GPU
parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')
parser.add_argument('--gpu', type=int, default=0, help='gpu')
parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)
parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')
parser.add_argument('--dataset_num', type=str, default='0', help='AIOps have 29 dataset,number:0-28')

# FITS
parser.add_argument('--train_mode', type=int,default=0)
parser.add_argument('--cut_freq', type=int,default=0)
parser.add_argument('--base_T', type=int,default=24)
parser.add_argument('--H_order', type=int,default=2)

# tsAMD
parser.add_argument('--n_block', type=int,default=1)
parser.add_argument('--mix_layer_num', type=int,default=2)
parser.add_argument('--mix_layer_scale', type=int,default=2)
parser.add_argument('--alpha', type=float,default=0.0)

# pathformer
parser.add_argument('--num_nodes', type=int, default=7)
parser.add_argument('--layer_nums', type=int, default=3)
parser.add_argument('--k', type=int, default=2, help='choose the Top K patch size at the every layer ')
parser.add_argument('--num_experts_list', type=list, default=[4, 4, 4])
parser.add_argument('--patch_size_list', nargs='+', type=int, default=[16,12,8,32,12,8,6,4,8,6,4,2])
parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')
# parser.add_argument('--revin', type=int, default=1, help='whether to apply RevIN')
parser.add_argument('--drop', type=float, default=0.1, help='dropout ratio')
# parser.add_argument('--embed', type=str, default='timeF',
#                     help='time features encoding, options:[timeF, fixed, learned]')
parser.add_argument('--residual_connection', type=int, default=1)
parser.add_argument('--batch_norm', type=int, default=0)

# heiyi
parser.add_argument('--save_path', type=str, default='/data/lrlresults/multiscale_patch', help='train start year')
# parser.add_argument('--is_training', type=int, default=1)
parser.add_argument('--train_start_year', type=str, default='2010', help='train start year')
parser.add_argument('--train_end_year', type=str, default='2019', help='train end year')
parser.add_argument('--val_start_year', type=str, default='2014', help='vali start year')
parser.add_argument('--use_original_feature', action='store_true', help='use automatic mixed precision training', default=False)
parser.add_argument('--kfold', action='store_true', help='use kfold', default=False)
parser.add_argument('--per20', action='store_true', help='use foldper20', default=False)
parser.add_argument('--num_fold', type=int, default=5, help='')
parser.add_argument('--pred_task', type=int, default=10, help='y5,y10,y20')
parser.add_argument('--lgb', action='store_true', help='use lgb regressor', default=False)
parser.add_argument('--output_channels', type=int,default=1)
parser.add_argument('--label_type', type=str,default='raw')

parser.add_argument('--seed', type=int, default=42, help='seed')
parser.add_argument('--single_fold', type=int, default=None, help='train single fold for parallel execution')
parser.add_argument('--fold_start', type=int, default=3, help='fold_start')
parser.add_argument('--fold_end', type=int, default=5, help='fold_end')
parser.add_argument('--gpu_list', type=str, default='6,7', help='GPU list for 5-fold parallel training, separated by comma')
parser.add_argument('--test_only', action='store_true', help='only run testing', default=False)

# å¹¶è¡Œè®­ç»ƒå‡½æ•°
def train_single_fold(fold_id, args_dict, setting):
    """å•ä¸ªfoldçš„è®­ç»ƒå‡½æ•°ï¼Œç”¨äºå¤šè¿›ç¨‹"""
    import torch
    import random
    import numpy as np
    from exp.exp_multiple_regression_fold_time import Exp_Multiple_Regression_Fold
    import os

    # --- å…³é”®ä¿®æ”¹ï¼šè®¾ç½® CUDA éš”ç¦» ---
    # è·å–åˆ†é…ç»™è¯¥è¿›ç¨‹çš„ç‰©ç† GPU ID
    assigned_gpu = args_dict['gpus'][fold_id-args_dict['fold_start']]  # æ³¨æ„ï¼šè¿™é‡Œè¦ä»å­—å…¸é‡Œå– gpus
    # é™åˆ¶è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°è¿™ä¸€å— GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = str(assigned_gpu)

    # é‡å»ºargså¯¹è±¡
    class Args:
        pass

    log_file_path = os.path.join(args_dict['train_log_dir'], f'fold_{fold_id + 1}_training.log')
    log_file = open(log_file_path, 'a', buffering=1)  # buffering=1 è¡¨ç¤ºè¡Œç¼“å†²ï¼Œå®æ—¶å†™å…¥
    original_stdout = sys.stdout

    # å°†å½“å‰è¿›ç¨‹çš„æ‰€æœ‰ print() è¾“å‡ºæŒ‡å‘æ–‡ä»¶
    sys.stdout = log_file
    # å¦‚æœå¸Œæœ›é”™è¯¯ä¿¡æ¯ä¹Ÿè¿›æ–‡ä»¶ï¼Œå–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Šï¼›å¦‚æœå¸Œæœ›æŠ¥é”™åœ¨å±å¹•æ˜¾ç¤ºï¼Œåˆ™ä¿ç•™æ³¨é‡Š
    # sys.stderr = log_file
    args = Args()
    for key, value in args_dict.items():
        setattr(args, key, value)

    # --- å…³é”®ä¿®æ”¹ï¼šé‡ç½®å†…éƒ¨ GPU ID ä¸º 0 ---
    # å› ä¸ºè®¾ç½®äº† CUDA_VISIBLE_DEVICESï¼Œç°åœ¨è¿™å°±å˜æˆäº†è¯¥è¿›ç¨‹çš„ç¬¬ 0 å·è®¾å¤‡
    args.gpu = 0
    args.device = torch.device("cuda:0")
    torch.set_num_threads(8)
    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    print(f'>>>>>>> Fold {fold_id + 1}: PID {os.getpid()} using Physical GPU {assigned_gpu} (Logical cuda:0) >>>>>>>')
    print(f"æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file_path}")
    # è®¾ç½®éšæœºç§å­
    fix_seed = args.seed
    random.seed(fix_seed)
    torch.manual_seed(fix_seed)
    np.random.seed(fix_seed)

    try:
        # åˆ›å»ºå®éªŒå¹¶è®­ç»ƒ
        exp = Exp_Multiple_Regression_Fold(args, single_fold=fold_id)
        exp.train(setting)
        print(f'>>>>>>> Fold {fold_id + 1} Finished Successfully <<<<<<<')
    except Exception as e:
        import traceback
        traceback.print_exc(file=log_file)
        sys.stderr.write(f"\n!!!! Fold {fold_id + 1} Error !!!! æŸ¥çœ‹æ—¥å¿—: {log_file_path}\n")
        traceback.print_exc()
    finally:
        # å…³é—­æ–‡ä»¶ï¼Œè™½ç„¶è¿›ç¨‹ç»“æŸä¼šè‡ªåŠ¨å…³é—­ï¼Œä½†æ˜¾å¼å…³é—­æ˜¯å¥½ä¹ æƒ¯
        log_file.close()

    return fold_id

def check_fold_complete(log_file, fold_id):
    """
    æ£€æŸ¥æŒ‡å®šfoldçš„æ—¥å¿—æ˜¯å¦åŒ…å«å®Œæˆæ ‡å¿—ï¼ˆå›ºå®šæ ¼å¼ï¼‰
    :param log_file: æ—¥å¿—æ–‡ä»¶ç»å¯¹è·¯å¾„
    :param fold_id: è¦æ£€æŸ¥çš„foldç´¢å¼•ï¼ˆ3/4ï¼‰
    :return: True=å®Œæˆï¼ŒFalse=æœªå®Œæˆ/æ—¥å¿—ä¸å­˜åœ¨
    """
    if not os.path.exists(log_file):
        return False

    # åŒ¹é…çš„æ ¸å¿ƒæ ‡å¿—ï¼ˆå¿…é¡»å’Œæ—¥å¿—è¾“å‡ºå®Œå…¨ä¸€è‡´ï¼‰
    fold_num = fold_id + 1  # fold3â†’Fold4ï¼Œfold4â†’Fold5
    complete_flag = f">>>>>>> Fold {fold_num} Finished Successfully <<<<<<<"

    count = 0
    try:
        # é€è¡Œè¯»å–ç»Ÿè®¡ï¼Œé¿å…å†…å­˜é—®é¢˜
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                if complete_flag in line:
                    count += 1
    except Exception as e:
        print(f"âš ï¸ è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {str(e)}")
        return False
    return count

def wait_serverB_folds(serverB_log_dir, wait_interval=300):
    """
    è½®è¯¢ç­‰å¾…æœåŠ¡å™¨Bçš„fold3/4è®­ç»ƒå®Œæˆ
    :param serverB_log_dir: æœåŠ¡å™¨Bæ—¥å¿—æ‰€åœ¨ç›®å½•ï¼ˆå…±äº«å­˜å‚¨è·¯å¾„ï¼‰
    :param wait_interval: è½®è¯¢é—´éš”ï¼ˆç§’ï¼Œå»ºè®®5åˆ†é’Ÿ=300ç§’ï¼‰
    """
    target_folds = [fold for fold in range(args.num_fold)]  # æœåŠ¡å™¨Bè´Ÿè´£çš„foldç´¢å¼•
    completed_folds = set()

    print(f"\n========== å¼€å§‹ç›‘æ§æœåŠ¡å™¨Bè®­ç»ƒè¿›åº¦ ==========")
    print(f"ç›‘æ§ç›®å½•ï¼š{serverB_log_dir}")
    print(f"å¾…ç›‘æ§foldï¼š{[f + 1 for f in target_folds]}")
    base_log_file = os.path.join(serverB_log_dir, 'fold_1_training.log')
    base_count = check_fold_complete(base_log_file, 0)
    print(f"åŸºå‡†è®¡æ•°ï¼ˆFold 1æ—¥å¿—å®Œæˆæ¬¡æ•°ï¼‰ï¼š{base_count}")
    while len(completed_folds) < len(target_folds):
        # æ£€æŸ¥æ¯ä¸ªfoldçš„æ—¥å¿—
        for fold_id in target_folds:
            if fold_id in completed_folds:
                continue

            log_file = os.path.join(serverB_log_dir, f'fold_{fold_id + 1}_training.log')
            current_count = check_fold_complete(log_file, fold_id)
            if current_count > 0 and (fold_id not in completed_folds):
                print(f"  Fold {fold_id + 1}: å½“å‰å®Œæˆæ¬¡æ•°={current_count}ï¼ŒåŸºå‡†={base_count}")
                # åˆ¤æ–­æ¡ä»¶ï¼šå½“å‰æ¬¡æ•° >= åŸºå‡†æ¬¡æ•°
            if current_count >= base_count:
                completed_folds.add(fold_id)
                print(f"âœ… Fold {fold_id + 1} è¾¾æˆåŒæ­¥æ¡ä»¶ï¼ˆ{current_count}/{base_count}ï¼‰")

        # è®¡ç®—æœªå®Œæˆçš„fold
        remaining = [f + 1 for f in target_folds if f not in completed_folds]
        if remaining:
            print(f"â³ æœªå®Œæˆfoldï¼š{remaining}ï¼Œ{wait_interval / 60:.1f}åˆ†é’Ÿåé‡è¯•...")
            time.sleep(wait_interval)

    print(f"ğŸ‰ æœåŠ¡å™¨Bæ‰€æœ‰foldè®­ç»ƒå®Œæˆï¼")
    return True

def summarize_fold_results(args, setting):
    """
    æ±‡æ€»æ‰€æœ‰foldçš„è®­ç»ƒç»“æœ
    """
    print(f"\næ±‡æ€»è®­ç»ƒç»“æœ: {args.save_path}")

    results = {}
    missing_folds = []
    missing_models = []

    # è¯»å–å„foldçš„ç»“æœ
    for fold in range(args.num_fold):
        result_file = f'{args.save_path}/fold_{fold + 1}_results.npy'
        model_file = os.path.join(args.checkpoints + '/' + setting, f'best_model_fold_{fold + 1}.pth')

        # æ£€æŸ¥ç»“æœæ–‡ä»¶
        if os.path.exists(result_file):
            try:
                fold_result = np.load(result_file, allow_pickle=True).item()
                results[fold] = fold_result
                print(f"\nâœ“ Fold {fold + 1} ç»“æœ:")
                print(f"  - Best Train Corr: {fold_result.get('best_train_corr', 'N/A'):.6f}")
                print(f"  - Best Val Loss:   {fold_result.get('best_val_loss', 'N/A'):.6f}")
                print(f"  - Best Val Corr:   {fold_result.get('best_val_corr', 'N/A'):.6f}")
                print(f"  - Best Val SR:     {fold_result.get('best_val_sr', 'N/A'):.6f}")
                print(f"  - Best Val Metric: {fold_result.get('best_val_metric', 'N/A'):.6f}")
                print(f"  - Nowcast Corr:    {fold_result.get('nowcast_corr', 'N/A'):.6f}")
            except Exception as e:
                print(f"\nÃ— Fold {fold + 1} ç»“æœæ–‡ä»¶è¯»å–å¤±è´¥: {e}")
                missing_folds.append(fold + 1)
        else:
            print(f"\nÃ— Fold {fold + 1} ç»“æœæ–‡ä»¶æœªæ‰¾åˆ°")
            missing_folds.append(fold + 1)

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        if not os.path.exists(model_file):
            print(f"  Ã— æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_file}")
            missing_models.append(fold + 1)
        else:
            file_size = os.path.getsize(model_file) / (1024 * 1024)  # MB
            print(f"  âœ“ æ¨¡å‹æ–‡ä»¶: {file_size:.2f} MB")

    # è®¡ç®—å¹³å‡å€¼
    if results:
        print("\n" + "=" * 60)
        print("å¹³å‡ç»“æœæ±‡æ€»:")
        print("=" * 60)

        metrics = ['best_train_corr', 'best_val_loss', 'best_val_corr',
                   'best_val_sr', 'best_val_metric', 'nowcast_corr']

        avg_results = {}
        for metric in metrics:
            values = [r.get(metric) for r in results.values() if r.get(metric) is not None]
            if values:
                values = [v.item() if hasattr(v, 'item') else v for v in values]
                mean_val = np.mean(values)
                std_val = np.std(values)
                avg_results[metric] = {'mean': mean_val, 'std': std_val}
                print(f"{metric:20s}: {mean_val:.6f} Â± {std_val:.6f}")

        # ä¿å­˜æ±‡æ€»ç»“æœ
        with open(f'{args.save_path}/_result_of_multiple_regression.txt', 'a') as f:
            f.write("\n" + "=" * 60 + "\n")
            f.write(f"5æŠ˜äº¤å‰éªŒè¯æ±‡æ€»ç»“æœ\n")
            f.write("=" * 60 + "\n\n")

            for fold, result in results.items():
                f.write(f"Fold {fold + 1}:\n")
                for metric in metrics:
                    val = result.get(metric, 'N/A')
                    if val != 'N/A':
                        val = val.item() if hasattr(val, 'item') else val
                        f.write(f"  {metric:20s}: {val:.6f}\n")
                f.write("\n")

            f.write("=" * 60 + "\n")
            f.write("å¹³å‡ç»“æœ:\n")
            f.write("=" * 60 + "\n")
            for metric in metrics:
                if metric in avg_results:
                    f.write(f"{metric:20s}: {avg_results[metric]['mean']:.6f}\n")

    # æ£€æŸ¥æ˜¯å¦å¯ä»¥å¼€å§‹æµ‹è¯•
    print("\n" + "=" * 60)
    if missing_folds or missing_models:
        if missing_folds:
            print(f"âš  è­¦å‘Š: ä»¥ä¸‹foldç¼ºå°‘ç»“æœæ–‡ä»¶: {missing_folds}")
        if missing_models:
            print(f"âš  è­¦å‘Š: ä»¥ä¸‹foldç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {missing_models}")
        print("å»ºè®®ç­‰å¾…æ‰€æœ‰foldè®­ç»ƒå®Œæˆåå†è¿›è¡Œæµ‹è¯•")
        print("=" * 60)
        return False
    else:
        print("âœ“ æ‰€æœ‰foldè®­ç»ƒå·²å®Œæˆï¼Œæ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼Œå¯ä»¥å¼€å§‹æµ‹è¯•")
        print("=" * 60)
        return True

if __name__ == '__main__':
    pip_path = sys.executable.replace("python3.11", "pip")
    result = subprocess.run([pip_path, 'freeze'], capture_output=True, text=True)
    dependencies = result.stdout
    with open('requirements.txt', 'w') as file:
        file.write(dependencies)
    print("å·²ç”Ÿæˆå®‰è£…åŒ…åˆ—è¡¨ï¼šrequirements.txt")
    # æ£€æŸ¥æ˜¯å¦åœ¨å®ˆæŠ¤è¿›ç¨‹ä¸­è¿è¡Œï¼Œé¿å…"daemonic processes are not allowed to have children"é”™è¯¯
    try:
        # ä½¿ç”¨ torch.multiprocessing.set_start_method æ›´å®‰å…¨
        torch.multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        # å¦‚æœå·²ç»è®¾ç½®ï¼Œè·³è¿‡
        pass

    args = parser.parse_args()
    args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False


    # args.use_multi_gpu=1

    if args.use_gpu and args.use_multi_gpu:
        args.dvices = args.devices.replace(' ', '')
        device_ids = args.devices.split(',')
        args.device_ids = [int(id_) for id_ in device_ids]
        args.gpu = args.device_ids[-1]



    # args.is_training = 0
    args.data_new = '5'
    # args.ticker_type = 2  #0,1,2(all)

    args.weight_decay = 1e-5
    args.drop_ratio = 0.1
    args.pct_start = 0.6
    args.label_type = 'res'
    args.feature_selection = False
    args.train_epochs = 60
    args.patience = 10
    # args.individual = True
    args.n_splits = 3
    args.dataset = 'heiyi'  # [ETTh1, ETTh2, ETTm1, ETTm2, weather, public, elc, traffic,AIOps]
    args.lradj = 'not'
    args.random_zero_prob = 0.0
    args.random_mask_prob = 0.0

    "/cpfs/dss/dev/fxi/project/flap01/cta/daily/daily_label5_20_all_data_202312.feather"
    '/cpfs/dss/dev/lxjie/lxj_results/daily_10_p9_price_Basis_202312.feather'
    #/cpfs/dss/dev/fxi/project/flap01/stock/daily/daily_label10_data_addMask_202312.h5
    args.data_path = '/cpfs/dss/dev/fxi/project/flap01/stock/daily/daily_label10_data_addMask_202312.h5' # daily å› å­é›†å’ŒåŸå§‹ç‰¹å¾
    # args.data_path = '/data/stock_daily_2005_2021.feather'
    args.data_type = 'daily'
    args.freq = 'd'
    args.learning_rate = 1e-5
    # args.data_path = '/data/downsample_data3/15min_label36_320_data.feather' # min åŸå§‹ç‰¹å¾
    # args.data_path = '/data/downsample_data3/15min_label320_factors.feather' # min15 å› å­é›†
    # args.data_type = 'min15'
    pred_task = 10

    args.grad_norm = False
    args.dropout = args.drop_ratio
    args.tau_hat_init = 0.0
    args.MLP_layers = 3
    args.MLP_hidden = 128
    # args.seq_len = 120
    #
    args.train_start_year = '2017'
    # args.train_end_year = '2022'
    # args.gpu = 0
    args.test_year = str(int(args.train_end_year)+1)
    args.device = torch.device(f"cuda:{args.gpu}")
    args.features = 'M' # long MS
    args.task_name = 'multiple_regression'  # [Long_term_forecasting, multiple_regression, predict_feature, classification]
    args.model = 'PatchTST'  # MHPatchTST, FC_MLP_layer, PatchTST, FC_MLP, PatchTST_C_group, FC_Conv, FITS,MTPatchTST,MTMLP,LSTM
    args.fold_type = 'time_fold' # k_fold,time_fold
    args.val = True
    args.enc_in = 10
    args.num_fold = 5
    args.epsilon = 2
    # args.delta=0.3
    save_path = f'/cpfs/dss/dev/lxjie/lxj_results/stock/fold/{args.fold_type}/phi_ret_mean/{args.data_type}_price_{args.label_type}_cross_section_sample/'

    if args.cut_freq == 0:
        args.cut_freq = int(args.seq_len // args.base_T + 1) * args.H_order + 10

    '''
        æ¯ä¸ªtickerçš„valå–20%åškfold, å› å­é›†å’ŒåŸå§‹ç‰¹å¾
    '''
    epsilon = 2
    args.is_training = 1
    i=0
    args.patch_size_list = np.array(args.patch_size_list).reshape(args.layer_nums, -1).tolist()
    # def set_seed(seed=42):
    #     random.seed(seed)
    #     np.random.seed(seed)
    #     torch.manual_seed(seed)
    #     if torch.cuda.is_available():
    #         torch.cuda.manual_seed_all(seed)
    #     torch.backends.cudnn.deterministic = True
    #     torch.backends.cudnn.benchmark = False

    if args.is_training:
        for args.batch_size in [512]:
            for args.tau_hat_init in [0.0,1.0,2.0,3.0,4.0,4.5]:
                    # for args.seq_len in [120]:
                        # for args.kernel_size in [[3,5,7],[3,7,15]]:
                        # for args.seq_len in [90,120]:
                # for args.seq_len in range(180, 210+30,30): # min15 720
                    # --mlp LSTM
                    # for args.MLP_layers in [3,4]:
                    #     for args.MLP_hidden in [64,128,256]:
                    # for args.MLP_layers in [5,6,7]:
                    #     for args.MLP_hidden in [256,512,1024]:
                    # for args.MLP_layers in [4]:
                    #     for args.MLP_hidden in [64]:
                        # --patchtst
                        #     for args.d_model in [128]:
                                args.d_ff = args.d_model*2
                        # for args.d_ff in [32,64,128]:
                                for args.patch_len in [16,8]:
                                    # i+=1
                                    # if i<=1:
                                    #     continue
                                    args.stride = args.patch_len//2
                                        # if args.patch_len==16 and args.stride in [8,12]:
                                        #     continue
                                    args.e_layers = 3
                                    print('Args in experiment:')
                                    print(args)
                                    if args.data_type == 'daily':
                                        if args.task_name == 'Long_term_forecasting':
                                            args.pred_task = pred_task
                                            args.pred_len = args.pred_task
                                        elif args.task_name == 'multiple_regression':
                                            args.pred_task = pred_task
                                            args.pred_len = 1
                                        elif args.task_name == 'predict_feature':
                                            args.pred_task = pred_task
                                            args.pred_len = 1
                                    elif args.data_type == 'min15':
                                        if args.task_name == 'Long_term_forecasting':
                                            args.pred_task = pred_task
                                            args.pred_len = args.pred_task
                                        elif args.task_name == 'multiple_regression' or args.task_name == 'classification':
                                            args.pred_task = pred_task
                                            args.pred_len = 1
                                        elif args.task_name == 'predict_feature':
                                            args.pred_task = pred_task
                                            args.pred_len = 1
                                # for args.pred_len in [1]:# [96, 192, 336, 720]
                                    # if args.model == 'TimesNet':
                                    #     args.pred_len = 0

                                    fix_seed = args.seed
                                    # fix_seed = 42
                                    random.seed(fix_seed)
                                    torch.manual_seed(fix_seed)
                                    np.random.seed(fix_seed)
                                    args.size = [args.seq_len, args.pred_len]
                                    if args.loss == 'MSE_with_weak':
                                        train_des = f"{args.model}_test_year{args.test_year}_tau_x{args.tau_hat_init}_kfold{args.kfold}_seq{args.seq_len}_pred{args.pred_len}_ep{args.train_epochs}_bs{args.batch_size}_early{args.patience}_lr{args.learning_rate}_wd{args.weight_decay}_"
                                    else:
                                        train_des = f"{args.model}_test_year{args.test_year}_kfold{args.kfold}_seq{args.seq_len}_pred{args.pred_len}_ep{args.train_epochs}_bs{args.batch_size}_early{args.patience}_lr{args.learning_rate}_wd{args.weight_decay}_"
                                    # model = Model(args)
                                    # train_des_pretrain = f"NNN_{args.data_new}_task_name{args.task_name}_ticker_type{0}{args.model}_test_year{args.test_year}_seq{args.seq_len}_pred{args.pred_len}_freq{args.freq}_ep{args.train_epochs}_bs{128}_early{args.patience}_lr{args.learning_rate}_wd{args.weight_decay}_"
                                    if args.model == 'FITS':
                                        model_des = f"nl{args.n_layer}_nh{args.n_head}_ne_{args.n_embd}_era_dp{args.drop_ratio}_{args.features}_inv{args.individual}_dmo{args.d_model}_dff{args.d_ff}_horder{args.H_order}"
                                    else:
                                        model_des = f"eps{args.epsilon}_nl{args.n_layer}_nh{args.n_head}_ne_{args.n_embd}_era_dp{args.drop_ratio}_{args.features}_inv{args.individual}_dmo{args.d_model}_dff{args.d_ff}"
                                    patching_des = f'_pl{args.patch_len}_sr{args.stride}_val{args.val}'
                                    setting = train_des + model_des + patching_des
                                    # if args.task_name == 'multiple_regression':
                                    args.save_path = os.path.join(save_path, f'y{args.pred_task}/{args.model}_{setting}')
                                    args.checkpoints = args.save_path
                                    args.logs_dir = args.save_path + f'/logs'
                                    args.train_log_dir = f'/cpfs/dss/dev/lxjie/hy_stock/hy_daily/logs/{args.loss}_sample_cross_section_{args.test_year}_dm{args.d_model}_sq{args.seq_len}'
                                    if not os.path.exists( args.train_log_dir):
                                        os.makedirs( args.train_log_dir, exist_ok=True)
                                    if not os.path.exists(args.save_path):
                                        os.makedirs(args.save_path)
                                    with open(f'{args.save_path}/_result_of_multiple_regression.txt', 'a') as file:
                                        file.write('Args in experiment:\n' + f'{args}\n\n')

                                    # åˆ¤æ–­æ˜¯å¦æ˜¯å•æŠ˜è®­ç»ƒæ¨¡å¼ï¼ˆé€šè¿‡--single_foldå‚æ•°æ§åˆ¶ï¼‰
                                    if args.single_fold is not None:
                                        # å•æŠ˜æ¨¡å¼ï¼šç›´æ¥è®­ç»ƒæŒ‡å®šçš„fold
                                        print(f'>>>>>>>start training fold {args.single_fold + 1} : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>>')
                                        Exp = Exp_Multiple_Regression_Fold
                                        exp = Exp(args, single_fold=args.single_fold)
                                        exp.train(setting)
                                        print(f'>>>>>>>fold {args.single_fold + 1} training completed<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')
                                    else:
                                        args.gpus = [int(gpu.strip()) for gpu in args.gpu_list.split(',')]
                                        if len(args.gpus) != args.fold_end-args.fold_start:
                                            raise ValueError(
                                                f"GPU list size ({len(args.gpus)}) must match num_fold ({args.num_fold}).")

                                        start_time = time.time()
                                        processes = []

                                        # 1. å¯åŠ¨è®­ç»ƒè¿›ç¨‹
                                        for fold_id in range(args.fold_start,args.fold_end):
                                            # ä¸ºå½“å‰ fold åˆ›å»ºå‚æ•°å‰¯æœ¬
                                            current_args = vars(args).copy()

                                            # ä¸ºå­è¿›ç¨‹åˆ†é… GPU
                                            current_args['gpu'] = args.gpus[fold_id-args.fold_start]

                                            # åˆ›å»º Process å®ä¾‹
                                            p = Process(
                                                target=train_single_fold,
                                                args=(fold_id, current_args, setting)  # ä¼ é€’å‚æ•°
                                            )
                                            # Process é»˜è®¤æ˜¯éå®ˆæŠ¤è¿›ç¨‹ï¼Œå¯ä»¥å¯åŠ¨ DataLoader çš„å­è¿›ç¨‹
                                            p.start()
                                            processes.append(p)

                                        print(
                                            f"âœ… All {args.num_fold} folds started. Waiting for training to complete...")

                                        # 2. ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
                                        for p in processes:
                                            p.join()  # é˜»å¡ä¸»è¿›ç¨‹ï¼Œç›´åˆ°æ‰€æœ‰å­è¿›ç¨‹ç»“æŸ
                                        if args.fold_start == 0:
                                            wait_serverB_folds(args.train_log_dir, wait_interval=300)

                                            end_time = time.time()
                                            print(f"Total time for all folds: {end_time - start_time:.2f} seconds.")

                                            # 3. ç»“æœæ±‡æ€»å’Œè‡ªåŠ¨æµ‹è¯• (åœ¨æ‰€æœ‰è¿›ç¨‹å®Œæˆåæ‰§è¡Œ)
                                            print(
                                                '\n================== Folds Finished. Summarizing Results... ==================\n')
                                            all_ready = summarize_fold_results(args, setting)

                                            if all_ready and not args.test_only:
                                                print(f'Training of {setting} finished. Auto-testing...')
                                                exp = Exp_Multiple_Regression_Fold(args)
                                                exp.test(setting)
                                        # æ¸…ç†æ˜¾å­˜
                                        torch.cuda.empty_cache()
    else:
        # æµ‹è¯•æ¨¡å¼ï¼šç¡®ä¿å‚æ•°ä¸è®­ç»ƒæ—¶ä¸€è‡´
        for args.batch_size in [512]:
            # æµ‹è¯•æ—¶ä¸éå†tau_hat_initï¼Œä½¿ç”¨é»˜è®¤å€¼æˆ–è®­ç»ƒæ—¶çš„å€¼
            # for args.tau_hat_init in [0.0, 1.0, 2.0, 3.0, 4.0, 4.5]:
                # for args.learning_rate in [1e-5]:
                    # for args.kernel_size in [[3,5,7],[3,7,15]]:
                    # for args.seq_len in [90,120]:
                    # for args.seq_len in range(180, 210+30,30): # min15 720
                    # --mlp LSTM
                    # for args.MLP_layers in [3,4]:
                    #     for args.MLP_hidden in [64,128,256]:
                    # for args.MLP_layers in [5,6,7]:
                    #     for args.MLP_hidden in [256,512,1024]:
                    # for args.MLP_layers in [4]:
                    #     for args.MLP_hidden in [64]:
                    # --patchtst
                    #     for args.d_model in [64,128]:
            args.d_ff = args.d_model * 2
            # for args.d_ff in [32,64,128]:
            for args.patch_len in [16, 32]:
                # i+=1
                # if i<=2:
                #     continue
                args.stride = args.patch_len // 2
                # if args.patch_len==16 and args.stride in [8,12]:
                #     continue
                args.e_layers = 3
                print('Args in experiment:')
                print(args)
                if args.data_type == 'daily':
                    if args.task_name == 'Long_term_forecasting':
                        args.pred_task = pred_task
                        args.pred_len = args.pred_task
                    elif args.task_name == 'multiple_regression':
                        args.pred_task = pred_task
                        args.pred_len = 1
                    elif args.task_name == 'predict_feature':
                        args.pred_task = pred_task
                        args.pred_len = 1
                elif args.data_type == 'min15':
                    if args.task_name == 'Long_term_forecasting':
                        args.pred_task = pred_task
                        args.pred_len = args.pred_task
                    elif args.task_name == 'multiple_regression' or args.task_name == 'classification':
                        args.pred_task = pred_task
                        args.pred_len = 1
                    elif args.task_name == 'predict_feature':
                        args.pred_task = pred_task
                        args.pred_len = 1
                # for args.pred_len in [1]:# [96, 192, 336, 720]
                # if args.model == 'TimesNet':
                #     args.pred_len = 0

                fix_seed = args.seed
                # fix_seed = 42
                random.seed(fix_seed)
                torch.manual_seed(fix_seed)
                np.random.seed(fix_seed)
                args.size = [args.seq_len, args.pred_len]
                # æµ‹è¯•æ—¶ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„è·¯å¾„å‘½åè§„åˆ™
                if args.loss == 'MSE_with_weak':
                    train_des = f"{args.model}_test_year{args.test_year}_tau_x{args.tau_hat_init}_kfold{args.kfold}_seq{args.seq_len}_pred{args.pred_len}_ep{args.train_epochs}_bs{args.batch_size}_early{args.patience}_lr{args.learning_rate}_wd{args.weight_decay}_"
                else:
                    train_des = f"{args.model}_test_year{args.test_year}_kfold{args.kfold}_seq{args.seq_len}_pred{args.pred_len}_ep{args.train_epochs}_bs{args.batch_size}_early{args.patience}_lr{args.learning_rate}_wd{args.weight_decay}_"
                # model = Model(args)
                # train_des_pretrain = f"NNN_{args.data_new}_task_name{args.task_name}_ticker_type{0}{args.model}_test_year{args.test_year}_seq{args.seq_len}_pred{args.pred_len}_freq{args.freq}_ep{args.train_epochs}_bs{128}_early{args.patience}_lr{args.learning_rate}_wd{args.weight_decay}_"
                if args.model == 'FITS':
                    model_des = f"nl{args.n_layer}_nh{args.n_head}_ne_{args.n_embd}_era_dp{args.drop_ratio}_{args.features}_inv{args.individual}_dmo{args.d_model}_dff{args.d_ff}_horder{args.H_order}"
                else:
                    model_des = f"eps{args.epsilon}_nl{args.n_layer}_nh{args.n_head}_ne_{args.n_embd}_era_dp{args.drop_ratio}_{args.features}_inv{args.individual}_dmo{args.d_model}_dff{args.d_ff}"
                patching_des = f'_pl{args.patch_len}_sr{args.stride}_val{args.val}'
                setting = train_des + model_des + patching_des
                # if args.task_name == 'multiple_regression':
                args.save_path = os.path.join(save_path, f'y{args.pred_task}/{args.model}_{setting}')
                args.checkpoints = args.save_path
                args.logs_dir = args.save_path + f'/logs'

                # æµ‹è¯•å‰æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
                if not os.path.exists(args.save_path):
                    print(f"é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒç»“æœè·¯å¾„: {args.save_path}")
                    print("è¯·ç¡®ä¿å·²å®Œæˆè®­ç»ƒï¼Œæˆ–æ£€æŸ¥è¶…å‚æ•°è®¾ç½®æ˜¯å¦ä¸è®­ç»ƒæ—¶ä¸€è‡´")
                    continue

                with open(f'{args.save_path}/_result_of_multiple_regression.txt', 'a') as file:
                    file.write('\n' + '='*60 + '\n')
                    file.write('Testing with Args:\n' + f'{args}\n\n')

                Exp = Exp_Multiple_Regression_Fold
                exp = Exp(args)  # set experiments
                print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
                exp.test(setting)
                torch.cuda.empty_cache()
